{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall zoish -y\n",
    "! pip install git+https://github.com/TorkamaniLab/zoish.git\n",
    "! pip install  scikit-learn numpy lightgbm==4.0.0  nest-asyncio==1.5.7 plotly==5.16.1 pandas xgboost lohrasb ray xgboost feature-engine hyperopt ipywidgets  argcomplete==3.1.1 catboost==1.2.1 --force-reinstall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "import logging\n",
    "import sys  # Added sys for version logging\n",
    "\n",
    "# Third-party libraries for data manipulation and computation\n",
    "import pandas as pd\n",
    "import numpy  # Added numpy for version logging\n",
    "\n",
    "# Scikit-learn libraries for modeling, metrics, and data transformation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn  \n",
    "\n",
    "# XGBoost for machine learning modeling\n",
    "import xgboost  # Added for version logging\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Feature Engine for imputation\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "# Custom libraries for feature selection\n",
    "from zoish.feature_selectors.shap_selectors import ShapFeatureSelector, ShapPlotFeatures\n",
    "import zoish  # Added for version logging\n",
    "\n",
    "# Ray for distributed computing and hyperparameter tuning\n",
    "import ray  # Added for version logging\n",
    "from ray import tune, air\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "# Import lohrasb\n",
    "import lohrasb \n",
    "\n",
    "# Custom libraries for logging and model building\n",
    "from lohrasb.best_estimator import BaseModel\n",
    "from zoish import logger  # Custom logging\n",
    "\n",
    "# IPython settings for better notebook interactivity\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Set logging level\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Log versions of key libraries for debugging and documentation\n",
    "print(f'Python version : {sys.version}')\n",
    "print(f'zoish version : {zoish.__version__}')\n",
    "print(f'sklearn version : {sklearn.__version__}')\n",
    "print(f'pandas version : {pd.__version__}')\n",
    "print(f'numpy version : {numpy.__version__}')\n",
    "print(f'xgboost version : {xgboost.__version__}')\n",
    "print(f'ray version : {ray.__version__}')\n",
    "print(f'lohrasb version : {lohrasb.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the dataset URL\n",
    "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data\"\n",
    "\n",
    "# Load the data\n",
    "# The Parkinsons Telemonitoring dataset from UCI ML Repository has biomedical voice measurements \n",
    "# from 42 people with early-stage Parkinson's disease. The goal is to use these measurements \n",
    "# to predict the UPDRS (Unified Parkinson's Disease Rating Scale) score, which is a widely used\n",
    "# clinical scale for the disease symptoms. Higher scores represent more severe symptoms.\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# Define the target variable - motor_UPDRS column which is a clinician's motor score \n",
    "y = df[\"motor_UPDRS\"]\n",
    "\n",
    "# Define the feature set - all the other columns in the dataset\n",
    "X = df.drop([\"subject#\",\"motor_UPDRS\", \"total_UPDRS\",\"test_time\"], axis=1)  # we remove \"total_UPDRS\" because it's another target column\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "# 80% of the data will be used for training, and 20% will be used for testing the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols =  X_train.select_dtypes(include=['float']).columns.tolist()\n",
    "\n",
    "estimator = XGBRegressor()\n",
    "# Define the search space\n",
    "param_space = {\n",
    "                \"max_depth\": tune.randint(5, 7),\n",
    "                \"gamma\" : tune.uniform(0.01,2),\n",
    "                 }\n",
    "# create search algorithm, check main documentation of Tune at https://docs.ray.io/en/latest/tune/api/suggestion.html\n",
    "search_alg = HyperOptSearch()\n",
    "\n",
    "# define kwargs for base model\n",
    "kwargs = {  \n",
    "    'kwargs':{# params for fit method  \n",
    "            'fit_tune_kwargs' :{\n",
    "            'sample_weight':None,\n",
    "            },\n",
    "            # params for TuneCV\n",
    "            'main_tune_kwargs' : {\n",
    "            'cv':3,\n",
    "            'scoring':'r2',\n",
    "            'estimator':estimator,\n",
    "            },\n",
    "            # kwargs of Tuner \n",
    "            'tuner_kwargs':{\n",
    "                'tune_config':tune.TuneConfig(\n",
    "                                    search_alg=search_alg,\n",
    "                                    mode='max',\n",
    "                                    metric='score',\n",
    "\n",
    "                                ),\n",
    "                'param_space':param_space,\n",
    "                'run_config':air.RunConfig(stop={\"training_iteration\": 20}),\n",
    "            \n",
    "            },}\n",
    "    \n",
    "}\n",
    "\n",
    "obj = BaseModel().optimize_by_tune(\n",
    "    **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline =Pipeline([\n",
    "            # int missing values imputers\n",
    "            ('floatimputer', MeanMedianImputer(\n",
    "                imputation_method='mean', variables=float_cols)),\n",
    "           \n",
    "\n",
    "\n",
    " ])\n",
    "\n",
    "\n",
    "pipeline.fit_transform(X_train,y_train)\n",
    "obj.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Selection using ShapFeatureSelector \n",
    "estimator_for_feature_selector = obj.best_estimator\n",
    "shap_feature_selector = ShapFeatureSelector(model=estimator_for_feature_selector, num_features=5, scoring='r2', direction='maximum', n_iter=10, cv=5, algorithm = 'auto')\n",
    "\n",
    "# Regressor model\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('feature_selection', shap_feature_selector),\n",
    "    ('regressor', regressor)\n",
    "    ]\n",
    "    )\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Output first 10 predictions\n",
    "print(y_test_pred[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check performance of the Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('r2 score : ')\n",
    "print(r2_score(y_test,y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Plots the feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_factory = ShapPlotFeatures(shap_feature_selector,type_of_plot='bar_plot')\n",
    "plot_factory.bar_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_factory.summary_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_factory.summary_plot_full()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.17 ('prod_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3e82f33fff0f86e33d8c3a2efbacef89e166d1ae679c5c81a5bfe456b45cdcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
