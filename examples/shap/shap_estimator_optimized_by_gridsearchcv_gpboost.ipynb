{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gpboost==1.2.6\n",
    "! pip install git+https://github.com/TorkamaniLab/zoish.git\n",
    "! pip install feature-engine category-encoders scikit-learn ipywidgets numpy pandas --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing built-in libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import sys  # For accessing system-specific parameters and functions\n",
    "import zoish  # Assuming it's a custom library for your project\n",
    "import sklearn  # For machine learning models\n",
    "import numpy  # For numerical computations\n",
    "import gpboost\n",
    "\n",
    "# Importing scikit-learn utilities for various ML tasks\n",
    "from sklearn.compose import ColumnTransformer  # For applying transformers to columns\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest classifier\n",
    "from sklearn.impute import SimpleImputer  # For handling missing data\n",
    "from sklearn.metrics import (  # For evaluating the model\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split  # For CV and splitting dataset\n",
    "from sklearn.pipeline import Pipeline  # For creating ML pipelines\n",
    "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
    "\n",
    "# Importing other third-party libraries\n",
    "from category_encoders import TargetEncoder  # For encoding categorical variables\n",
    "from zoish.feature_selectors.shap_selectors import (  # For feature selection and visualization\n",
    "    ShapFeatureSelector,\n",
    "    ShapPlotFeatures,\n",
    ")\n",
    "import logging  # For logging events and errors\n",
    "\n",
    "# Configuring logging settings\n",
    "from zoish import logger  # Assuming it's a custom logger from zoish\n",
    "logger.setLevel(logging.ERROR)  # Set logging level to ERROR\n",
    "\n",
    "# Importing feature imputation library\n",
    "from feature_engine.imputation import MeanMedianImputer  # For imputing mean/median\n",
    "\n",
    "# Re-setting logging level (this seems redundant, consider keeping only one)\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Printing versions of key libraries for debugging and documentation\n",
    "print(f'Python version : {sys.version}')\n",
    "print(f'zoish version : {zoish.__version__}')\n",
    "print(f'sklearn version : {sklearn.__version__}')\n",
    "print(f'pandas version : {pd.__version__}')  # Using the alias for pandas\n",
    "print(f'numpy version : {numpy.__version__}')\n",
    "print(f'gpboost version : {gpboost.__version__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Audiology (Standardized) Data Set\n",
    "###### https://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29\n",
    "\n",
    "\n",
    "#### Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpboost as gpb\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "# Generate classification data for mixed effect model\n",
    "X, y = make_classification(n_samples=1000, n_features=100, n_informative=50, n_redundant=0, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "# Generate random effects\n",
    "n_groups = 5 # Reduced number of groups for simplicity\n",
    "groups = np.random.choice(n_groups, size=X.shape[0])\n",
    "\n",
    "# Define fixed group effects\n",
    "group_effects = np.random.normal(0, 1, n_groups)  # Random effects for each group\n",
    "random_effects = group_effects[groups]\n",
    "\n",
    "# Adjust y based on random effects\n",
    "y = np.where(y + random_effects > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the feature pipeline steps:\n",
    "Here, we use an untuned gpb.GPBoostClassifier model with the ShapFeatureSelector.In the next section, we will repeat the same process but with a tuned gpb.GPBoostClassifier. The aim is to demonstrate that a better estimator can yield improved results when used with the ShapFeatureSelector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test,groups_train,groups_test = train_test_split(\n",
    "    X, y, groups, test_size=0.33,  random_state=42\n",
    ")\n",
    "\n",
    "gp_model_train = gpb.GPModel(group_data=groups_train, likelihood=\"gaussian\")\n",
    "gp_model_train.set_prediction_data(group_data_pred=groups_train)\n",
    "\n",
    "bst = gpb.GPBoostClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',  # 'binary' is for binary classification\n",
    "    n_estimators=100,  # Equivalent to num_boost_round in gpboost.train\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "bst.fit(X_train, y_train,gp_model=gp_model_train)\n",
    "shap_feature_selector = ShapFeatureSelector(\n",
    "        bst,\n",
    "        n_iter=10,\n",
    "        scoring=\"f1\",\n",
    "        direction=\"maximum\",\n",
    "        cv=KFold(n_splits=2, shuffle=True),\n",
    "        # for gpboost this should be False\n",
    "        use_faster_algorithm=False,\n",
    "        num_features=50,\n",
    "        shap_fast_tree_explainer_kwargs={'algorithm':'v2'},\n",
    "        # pred_contrib = True  is important for feature selection by shap\n",
    "        predict_params={'group_data_pred':groups_train,'fixed_effects_pred':None,'pred_contrib':True},\n",
    ")\n",
    "\n",
    "        \n",
    "# Define pre-processing for numeric columns (float and integer types)\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define pre-processing for categorical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', TargetEncoder(handle_missing='return_nan'))])\n",
    "\n",
    "# Combine preprocessing into one column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature Selection using ShapSelector \n",
    "feature_selection = shap_feature_selector \n",
    "\n",
    "\n",
    "# Create a pipeline that combines the preprocessor with a feature selection and a classifier\n",
    "transformers = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('feature_selection', feature_selection),\n",
    "                           #('classifier', classifier)\n",
    "                           ]\n",
    "                           \n",
    "                           )\n",
    "\n",
    "# Fit the model\n",
    "XT_train=transformers.fit_transform(X_train, y_train)\n",
    "XT_test=transformers.transform(X_test)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "bst.fit(XT_train, y_train,gp_model=gp_model_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = bst.predict(X=XT_test, group_data_pred=groups_test)\n",
    "\n",
    "# Output first 10 predictions\n",
    "print(y_test_pred[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check performance of the Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"F1 score : \")\n",
    "print(f1_score(y_test, y_test_pred,average='micro'))\n",
    "print(\"Classification report : \")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"Confusion matrix : \")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shap related plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the feature importance\n",
    "plot_factory = ShapPlotFeatures(shap_feature_selector) \n",
    "plot_factory.summary_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_factory.summary_plot_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the feature importance\n",
    "plot_factory.bar_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_factory.bar_plot_full()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.17 ('prod_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3e82f33fff0f86e33d8c3a2efbacef89e166d1ae679c5c81a5bfe456b45cdcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
