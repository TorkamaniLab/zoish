{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn libraries for model selection, metrics, pipeline, impute, preprocessing, compose, and ensemble\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV,  train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Other libraries\n",
    "from category_encoders import TargetEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from zoish.feature_selectors.shap_selectors import ShapFeatureSelector, ShapPlotFeatures\n",
    "import logging\n",
    "from zoish import logger\n",
    "logger.setLevel(logging.ERROR)\n",
    "from feature_engine.imputation import (\n",
    "     MeanMedianImputer\n",
    "    )\n",
    "\n",
    "# Set logging level\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Audiology (Standardized) Data Set\n",
    "###### https://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29\n",
    "\n",
    "\n",
    "#### Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urldata = \"https://archive.ics.uci.edu/ml/machine-learning-databases/lymphography/lymphography.data\"\n",
    "urlname = \"https://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.names\"\n",
    "# column names\n",
    "col_names = [\n",
    "    \"class\",\n",
    "    \"lymphatics\",\n",
    "    \"block of affere\",\n",
    "    \"bl. of lymph. c\",\n",
    "    \"bl. of lymph. s\",\n",
    "    \"by pass\",\n",
    "    \"extravasates\",\n",
    "    \"regeneration of\",\n",
    "    \"early uptake in\",\n",
    "    \"lym.nodes dimin\",\n",
    "    \"lym.nodes enlar\",\n",
    "    \"changes in lym.\",\n",
    "    \"defect in node\",\n",
    "    \"changes in node\",\n",
    "    \"special forms\",\n",
    "    \"dislocation of\",\n",
    "    \"exclusion of no\",\n",
    "    \"no. of nodes in\",\n",
    "\n",
    "]\n",
    "\n",
    "data = pd.read_csv(urldata,names=col_names)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define labels and train-test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data.loc[(data[\"class\"] == 1) | (data[\"class\"] == 2), \"class\"] = 0\n",
    "data.loc[data[\"class\"] == 3, \"class\"] = 1\n",
    "data.loc[data[\"class\"] == 4, \"class\"] = 2\n",
    "data[\"class\"] = data[\"class\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != \"class\"]\n",
    "y = data.loc[:, data.columns == \"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33,  random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the feature pipeline steps:\n",
    "Here, we use an untuned XGBClassifier model with the ShapFeatureSelector.In the next section, we will repeat the same process but with a tuned XGBClassifier. The aim is to demonstrate that a better estimator can yield improved results when used with the ShapFeatureSelector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_for_feature_selector= XGBClassifier()     \n",
    "estimator_for_feature_selector.fit(X_train, y_train)\n",
    "shap_feature_selector = ShapFeatureSelector(model=estimator_for_feature_selector, num_features=5, cv = 5, scoring='accuracy', direction='maximum', n_iter=10, algorithm='auto')\n",
    "        \n",
    "# Define pre-processing for numeric columns (float and integer types)\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define pre-processing for categorical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', TargetEncoder(handle_missing='return_nan'))])\n",
    "\n",
    "# Combine preprocessing into one column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Feature Selection using ShapSelector \n",
    "feature_selection = shap_feature_selector \n",
    "\n",
    "# Classifier model\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Create a pipeline that combines the preprocessor with a feature selection and a classifier\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('feature_selection', feature_selection),\n",
    "                           ('classifier', classifier)])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Output first 10 predictions\n",
    "print(y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check performance of the Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"F1 score : \")\n",
    "print(f1_score(y_test, y_test_pred,average='micro'))\n",
    "print(\"Classification report : \")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"Confusion matrix : \")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Use better estimator:\n",
    "In this iteration, we will utilize the optimally tuned estimator with the ShapFeatureSelector, which is expected to yield improved results.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols =  X_train.select_dtypes(include=['int']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the XGBClassifier\n",
    "xgb_clf = XGBClassifier()\n",
    "\n",
    "# Define the parameter grid for XGBClassifier\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [ 4, 5],\n",
    "    'min_child_weight': [1, 2, 3],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "# Define the scoring function\n",
    "scoring = make_scorer(f1_score, average='micro')  # Use 'micro' average in case of multiclass target\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(xgb_clf, param_grid, cv=5, scoring=scoring, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Fit the GridSearchCV object\n",
    "estimator_for_feature_selector= grid_search.best_estimator_ \n",
    "shap_feature_selector = ShapFeatureSelector(model=estimator_for_feature_selector, num_features=5, scoring='accuracy', algorithm='auto',cv = 5, n_iter=10, direction='maximum')\n",
    "\n",
    "\n",
    "pipeline =Pipeline([\n",
    "            # int missing values imputers\n",
    "            ('floatimputer', MeanMedianImputer(\n",
    "                imputation_method='mean', variables=int_cols)),\n",
    "           \n",
    "            ('shap_feature_selector', shap_feature_selector),\n",
    "            ('classfier', RandomForestClassifier(n_estimators=100))\n",
    "\n",
    "\n",
    " ])\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Output first 10 predictions\n",
    "print(y_test_pred[:10])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance has improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"F1 score : \")\n",
    "print(f1_score(y_test, y_test_pred,average='micro'))\n",
    "print(\"Classification report : \")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"Confusion matrix : \")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shap related plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the feature importance\n",
    "plot_factory = ShapPlotFeatures(shap_feature_selector) \n",
    "plot_factory.summary_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_factory.summary_plot_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the feature importance\n",
    "plot_factory.bar_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_factory.bar_plot_full()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fd4a9ad6613f01435523ab5e4297db981c1d87a44e43e252d0e314391da6eb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
